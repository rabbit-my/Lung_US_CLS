import argparse
import os
import pandas as pd
import numpy as np
from sklearn.model_selection import GroupKFold
from sklearn.linear_model import Lasso, ElasticNet
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.metrics import roc_auc_score, accuracy_score, recall_score, confusion_matrix
import optuna
from optuna.samplers import TPESampler
import logging
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®æ—¥å¿—
def setup_logger():
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = f"final_ae_seg_{timestamp}.log"
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger()

logger = setup_logger()

# å‚æ•°è§£æ
def parse_args():
    parser = argparse.ArgumentParser(description='Lung US Classification Pipeline')
    
    # æ•°æ®å‚æ•°
    parser.add_argument('--data_dir', type=str, 
                       default='/root/data/lungUS_cls/autoencoder_fea_labeled/seg/',
                       help='Directory containing the fold data')
    
    # ç‰¹å¾é€‰æ‹©å‚æ•° (ä¿ç•™å‚æ•°ä½†å®é™…ä¸ä½¿ç”¨)
    parser.add_argument('--feature_selection', type=str, 
                       default='none', choices=['lasso', 'elasticnet', 'none'],
                       help='[Unused] Feature selection method')
    parser.add_argument('--correlation_threshold', type=float, 
                       default=0.9, help='[Unused] Threshold for removing correlated features')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--model', type=str, 
                       default='xgboost', choices=['xgboost', 'svm', 'randomforest'],
                       help='Model to use for classification')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--inner_folds', type=int, 
                       default=10, help='Number of inner CV folds')
    parser.add_argument('--n_trials', type=int, 
                       default=50, help='Number of Optuna trials')
    
    # è¾“å‡ºå‚æ•°
    parser.add_argument('--output_dir', type=str,
                       default='./ae_results/seg',
                       help='Directory to save prediction results')
    
    return parser.parse_args()

# è®¡ç®—é¢å¤–æŒ‡æ ‡
def calculate_metrics(y_true, y_pred_prob):
    """è®¡ç®—ACCã€çµæ•åº¦ã€ç‰¹å¼‚æ€§"""
    y_pred = (y_pred_prob >= 0.5).astype(int)
    acc = accuracy_score(y_true, y_pred)
    
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    sensitivity = tp / (tp + fn)  # çµæ•åº¦
    specificity = tn / (tn + fp)  # ç‰¹å¼‚æ€§
    
    return acc, sensitivity, specificity

# æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
class DataLoader:
    def __init__(self, data_dir):
        self.data_dir = data_dir
        
    def load_fold_data(self, fold):
        train_path = os.path.join(self.data_dir, f'fold{fold}_train_labeled.csv')
        test_path = os.path.join(self.data_dir, f'fold{fold}_test_labeled.csv')
        
        train_df = pd.read_csv(train_path)
        test_df = pd.read_csv(test_path)
        
        logger.info(f"\n{'='*50}")
        logger.info(f"ğŸ“Š Fold {fold} Data Summary:")
        logger.info(f"Train shape: {train_df.shape}, Test shape: {test_df.shape}")
        logger.info(f"Train class distribution:\n{train_df['label'].value_counts()}")
        logger.info(f"Test class distribution:\n{test_df['label'].value_counts()}")
        
        return train_df, test_df
    
    def preprocess_data(self, df):
        # ä¿å­˜æ ‡è¯†åˆ—
        id_cols = ['id', 'img_path']
        existing_id_cols = [col for col in id_cols if col in df.columns]
        ids = df[existing_id_cols].copy() if existing_id_cols else None
        
        # åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾
        label_col = 'label'
        if label_col not in df.columns:
            raise ValueError(f"Label column '{label_col}' not found in DataFrame")
        
        # å‡†å¤‡è¦åˆ é™¤çš„åˆ—ï¼ˆç¡®ä¿å®ƒä»¬å­˜åœ¨ï¼‰
        cols_to_remove = [label_col] + existing_id_cols
        cols_to_remove = [col for col in cols_to_remove if col in df.columns]
        
        X = df.drop(columns=cols_to_remove)
        y = df[label_col]
        
        # ä»…ä¿ç•™æ•°å€¼ç±»å‹ç‰¹å¾
        numeric_cols = X.select_dtypes(include=['number']).columns
        X = X[numeric_cols]
        
        logger.info(f"ğŸ“Š é¢„å¤„ç†åç»´åº¦: {X.shape}")
        logger.debug(f"å‰©ä½™ç‰¹å¾æ ·ä¾‹: {list(X.columns[:5])}...")
        
        return ids, X, y

# ç‰¹å¾å·¥ç¨‹ (ç®€åŒ–ç‰ˆï¼Œä¸åšä»»ä½•å¤„ç†)
class FeatureEngineer:
    def process_features(self, X_train, X_test, y_train):
        """
        ç®€åŒ–ç‰ˆç‰¹å¾å·¥ç¨‹ï¼š
        1. ä¸è¿›è¡Œæ ‡å‡†åŒ–
        2. ä¸ç§»é™¤ç›¸å…³ç‰¹å¾
        3. ä¸è¿›è¡Œç‰¹å¾é€‰æ‹©
        ç›´æ¥è¿”å›åŸå§‹ç‰¹å¾
        """
        logger.info("ğŸš« è·³è¿‡ç‰¹å¾å·¥ç¨‹æ­¥éª¤ï¼ˆæ— æ ‡å‡†åŒ–/ç‰¹å¾é€‰æ‹©ï¼‰")
        return X_train, X_test

# æ¨¡å‹è®­ç»ƒå’Œè°ƒä¼˜
class ModelTrainer:
    def __init__(self, model_type, inner_folds, n_trials):
        self.model_type = model_type
        self.inner_folds = inner_folds
        self.n_trials = n_trials
        self.best_params = None

        self.fixed_params = {
            'xgboost': {'random_state': 42},
            'randomforest': {'random_state': 42, 'bootstrap': True},
            'svm': {'probability': True, 'random_state': 42}
        }
    
    def objective(self, trial, X, y, groups):
        if self.model_type == 'xgboost':
            params = {
                'n_estimators': trial.suggest_int('n_estimators', 50, 500),
                'max_depth': trial.suggest_int('max_depth', 3, 10),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
                'subsample': trial.suggest_float('subsample', 0.6, 1.0),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
                'gamma': trial.suggest_float('gamma', 0, 5),
                'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
                'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),
                'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),
                'random_state': 42
            }
            model = XGBClassifier(**params)

        elif self.model_type == 'randomforest':
            params = {
                'n_estimators': trial.suggest_int('n_estimators', 50, 500),
                'max_depth': trial.suggest_int('max_depth', 3, 20),
                'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),
                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 5),
                'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2']),
                'bootstrap': True,
                'random_state': 42
            }
            model = RandomForestClassifier(**params)
        elif self.model_type == 'svm':
            kernel = trial.suggest_categorical('kernel', ['linear', 'rbf'])
            params = {
                'C': trial.suggest_float('C', 0.1, 10, log=True),
                'kernel': kernel,
                'gamma': trial.suggest_float('gamma', 1e-4, 1, log=True) if kernel == 'rbf' else 'scale',
                'probability': True,
                'random_state': 42
            }
            model = SVC(**params)
        
        # å†…å±‚äº¤å‰éªŒè¯
        group_kfold = GroupKFold(n_splits=self.inner_folds)
        auc_scores = []
        
        for train_idx, val_idx in group_kfold.split(X, y, groups):
            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
            
            model.fit(X_train, y_train)
            y_pred = model.predict_proba(X_val)[:, 1]
            auc = roc_auc_score(y_val, y_pred)
            auc_scores.append(auc)
        
        return np.mean(auc_scores)
    
    def tune_model(self, X, y, groups):
        study = optuna.create_study(direction='maximize', sampler=TPESampler(seed=42))
        study.optimize(lambda trial: self.objective(trial, X, y, groups), 
                        n_trials=self.n_trials)
        
        self.best_params = study.best_params
        
        logger.info(f"ğŸ¯ Best parameters: {self.best_params}")
        logger.info(f"Best AUC: {study.best_value:.4f}")
        
        return study.best_params
    
    def train_final_model(self, X_train, y_train, X_test, y_test):
        if self.model_type == 'xgboost':
            model = XGBClassifier(**self.best_params, random_state=42)
        elif self.model_type == 'randomforest':
            model = RandomForestClassifier(**self.best_params, random_state=42)
        elif self.model_type == 'svm':
            model = SVC(**self.best_params, probability=True, random_state=42)
        
        model.fit(X_train, y_train)
        
        # è®­ç»ƒé›†æ€§èƒ½
        train_pred = model.predict_proba(X_train)[:, 1]
        train_auc = roc_auc_score(y_train, train_pred)
        
        # æµ‹è¯•é›†æ€§èƒ½
        test_pred = model.predict_proba(X_test)[:, 1]
        test_auc = roc_auc_score(y_test, test_pred)
        
        # è®¡ç®—é¢å¤–æŒ‡æ ‡
        test_acc, test_sens, test_spec = calculate_metrics(y_test, test_pred)
        
        logger.info(f"ğŸ“Š Final Model Performance:")
        logger.info(f"Train AUC: {train_auc:.4f}")
        logger.info(f"Test AUC: {test_auc:.4f}, ACC: {test_acc:.4f}, Sensitivity: {test_sens:.4f}, Specificity: {test_spec:.4f}")
        
        return model, test_pred, test_auc, test_acc, test_sens, test_spec

# ä¸»æµç¨‹
def main():
    args = parse_args()
    logger.info(f"ğŸš€ Starting pipeline with parameters:\n{args.__dict__}")
    logger.info("âš ï¸ æ³¨æ„: å·²ç¦ç”¨ç‰¹å¾å·¥ç¨‹ï¼ˆæ— æ ‡å‡†åŒ–/ç‰¹å¾é€‰æ‹©ï¼‰")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    data_loader = DataLoader(args.data_dir)
    # ä½¿ç”¨ç®€åŒ–ç‰ˆç‰¹å¾å·¥ç¨‹
    feature_engineer = FeatureEngineer()
    model_trainer = ModelTrainer(args.model, args.inner_folds, args.n_trials)
    
    fold_results = []
    all_test_results = []
    
    # å¤–å±‚5æŠ˜äº¤å‰éªŒè¯
    for fold in range(1, 6):
        logger.info(f"\n{'='*50}")
        logger.info(f"ğŸŒŸ Processing Fold {fold}")
        
        # åŠ è½½æ•°æ®
        train_df, test_df = data_loader.load_fold_data(fold)
        
        # é¢„å¤„ç†
        ids_test, X_train, y_train = data_loader.preprocess_data(train_df)
        ids_test, X_test, y_test = data_loader.preprocess_data(test_df)
        
        # ç‰¹å¾å·¥ç¨‹ (ç®€åŒ–ç‰ˆï¼Œç›´æ¥è¿”å›åŸå§‹ç‰¹å¾)
        X_train_processed, X_test_processed = feature_engineer.process_features(
            X_train, X_test, y_train
        )
        
        # ç¡®ä¿åŒä¸€ä¸ªidä¸ä¼šåŒæ—¶å‡ºç°åœ¨è®­ç»ƒå’ŒéªŒè¯é›†
        groups = train_df['id'].values
        
        # æ¨¡å‹è°ƒä¼˜
        logger.info("ğŸ” Starting hyperparameter tuning...")
        model_trainer.tune_model(X_train_processed, y_train, groups)
        
        # è®­ç»ƒæœ€ç»ˆæ¨¡å‹
        logger.info("ğŸ‹ï¸ Training final model...")
        model, test_pred, test_auc, test_acc, test_sens, test_spec = model_trainer.train_final_model(
            X_train_processed, y_train, 
            X_test_processed, y_test
        )
        
        # ä¿å­˜æµ‹è¯•é›†é¢„æµ‹ç»“æœ
        test_results = pd.DataFrame({
            'fold': fold,
            'id': test_df['id'],
            'img_path': test_df['img_path'],
            'true_label': y_test.values,
            'pred_prob': test_pred
        })
        csv_path = os.path.join(args.output_dir, f'fold{fold}_test_predictions.csv')
        test_results.to_csv(csv_path, index=False)
        logger.info(f"ğŸ’¾ Saved test predictions to: {csv_path}")
        
        # è®°å½•å½“å‰æŠ˜çš„ç»“æœ
        fold_results.append({
            'fold': fold,
            'test_auc': test_auc,
            'test_acc': test_acc,
            'test_sens': test_sens,
            'test_spec': test_spec
        })
        
        # ä¿å­˜æ‰€æœ‰ç»“æœç”¨äºæœ€ç»ˆæ±‡æ€»
        all_test_results.append(test_results)
    
    # è®¡ç®—5æŠ˜æŒ‡æ ‡çš„å¹³å‡å€¼å’Œæ ‡å‡†å·®
    fold_metrics = pd.DataFrame(fold_results)
    metrics_summary = fold_metrics.describe().loc[['mean', 'std']]

    logger.info(f"\n{'='*50}")
    logger.info("ğŸ“Š 5-Fold Cross-Validation Metrics Summary:")
    logger.info(metrics_summary)

    # ä¿å­˜è¯¦ç»†æŒ‡æ ‡å’Œæ±‡æ€»
    detailed_metrics_path = os.path.join(args.output_dir, 'detailed_fold_metrics.csv')
    fold_metrics.to_csv(detailed_metrics_path, index=False)
    logger.info(f"ğŸ’¾ Saved detailed fold metrics to: {detailed_metrics_path}")

    summary_metrics_path = os.path.join(args.output_dir, 'summary_metrics.csv')
    metrics_summary.to_csv(summary_metrics_path)
    logger.info(f"ğŸ’¾ Saved summary metrics to: {summary_metrics_path}")

    # æœ€ç»ˆç»“æœæŠ¥å‘Š
    logger.info(f"\n{'='*50}")
    logger.info("ğŸ‰ Final 5-Fold Performance:")
    logger.info(f"AUC: {metrics_summary.loc['mean', 'test_auc']:.4f} Â± {metrics_summary.loc['std', 'test_auc']:.4f}")
    logger.info(f"ACC: {metrics_summary.loc['mean', 'test_acc']:.4f} Â± {metrics_summary.loc['std', 'test_acc']:.4f}")
    logger.info(f"Sensitivity: {metrics_summary.loc['mean', 'test_sens']:.4f} Â± {metrics_summary.loc['std', 'test_sens']:.4f}")
    logger.info(f"Specificity: {metrics_summary.loc['mean', 'test_spec']:.4f} Â± {metrics_summary.loc['std', 'test_spec']:.4f}")


if __name__ == "__main__":
    main()

